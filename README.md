# SPECTER as a Sentence-level Representation Model

Universal sentence representation is a hot topic in NLP research, because it provides a generic representation for a sentence that is suitable for all kinds of downstream NLP tasks. In this report, we have introduced a new candidate model to generate a universal sentence-level representations. Although the model SPECETER has been developed to generate a document-level representation, we proved that it has a very good performance as a sentence-level representation model when used in semantic textual similarity tasks. This performance is proved that it does not only compete the most-used sentence-level representation models, but also outperforms famous ones like: GloVe and SkipThought. However, we also prove that SPECTER does not have the same good performance when it comes to classification tasks. Therefore, as a future work, we have proposed to develop a new model that is inspired from SPECTER model. The new model will apply the same training triblets concept and the same function loss. However, it will be trained from the beginning on generating a sentence-level representation. In this report we have introduced a new dataset suitable for the model training, derived from the SNLI and Multi-NLI datasets where each element in this dataset consists of an anchor sentence, a positive sentence represents the same meaning, and a negative sentence represents opposite or different meaning. Also, we have introduced two types of negative samples: easy negative and hard negative, which is expected to make the model more precise. The next step is just to train the model using the developed dataset, and evaluate it the same way we evaluated the SPECTER model.

# Project Details
For detailed information on the study and the design choices refer to DOCUMENTATION.md.